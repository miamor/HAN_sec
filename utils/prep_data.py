"""
Prepare data
From report.json create:
    3 files for meta_path:
        proc_file.txt
        proc_net.txt
        proc_reg.txt
        proc_procapi.txt
    3 files for objects (nodes):
        proc.txt
        file.txt
        reg.txt

+ Meta_path data file format:
    n lines for n paths of this type in graph, each line:
        id_of_object_type_i \t id_of_object_type_j

+ proc object data file format:
    n lines for n nodes:
        id \t proc_name \t proc_path_severity \t regkey_written_severity \t dll_loaded_severity \t connects_host_severity \t

+ reg/net/file/procapi object data file format:
    - Show details of the api that links to it,
    - If n api falls into category registry/network/file/process, then there will be n reg/net/file/procapi nodes, each represents the characteristics of corresponding api.
        * note that procapi object is different from proc object.
    - If one api (with the same characteristics after calculating) is called multiple times, insert it once into the nodelist, later in the meta_path file links to the existed node's id.
        id \t api_name \t time \t arguments_severity \t flags_severity
            reg args eg:
                        "arguments": {
                            "key_handle": "0x00000070",
                            "desired_access": "0x00020019",
                            "regkey": "HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Nls\\ExtendedLocale"
                        },
            proc args eg:
                        "arguments": {
                            "section_handle": "0x00000074",
                            "object_handle": "0x00000000",
                            "desired_access": "0x000f0005",
                            "protection": 2,
                            "section_name": "",
                            "file_handle": "0x00000070"
                        },
            file args eg:
                        "arguments": {
                            "create_disposition": 1,
                            "file_handle": "0x00000070",
                            "filepath": "C:\\Windows\\Globalization\\Sorting\\sortdefault.nls",
                            "desired_access": "0x80100080",
                            "file_attributes": 128,
                            "filepath_r": "\\??\\C:\\Windows\\Globalization\\Sorting\\sortdefault.nls",
                            "create_options": 96,
                            "status_info": 1,
                            "share_access": 1
                        },
"""

import os
import json
import numpy as np
import scipy.sparse as sp
from .utils import indices_to_one_hot, label_encode_onehot, sample_mask, preprocess_adj, preprocess_features, save_pickle, load_pickle, save_txt, care_APIs
from dgl import DGLGraph
import math
import random
from utils.constants import *
import shutil

import torch
import torch.nn as nn

from graphviz import Digraph
import networkx as nx
import matplotlib.pyplot as plt


class PrepareData(object):
    DATA_OUT_PATH = ''  # data root dir
    # folder contains pickle file (save data in dortmund format)
    pickle_folder = ''

    reports_parent_dir_path = ''  # path to report.json generated by cuckoo
    behavior = {}  # behavior extracted from report.json in json format
    # consider only apis that fall into these categories
    # allow_cat = ['network', 'file', 'registry', 'process']
    allow_cat = ['file', 'process', 'registry']

    use_interesting_apis = None
    mapping_labels = {'benign': 0, 'malware': 1}

    nb_attributes = 100  # size of feature space
    edges = None
    nodes_labels = []
    edges_labels = []
    graphs_labels = []

    graphs_dict = dict()
    graphs_name_to_label = dict()
    graphs = []
    graphs_names = []
    graphs_viz = dict()

    final_json_data = {
        'nodes': [],
        'paths': []
    }
    final_json_path = ''  # path to data.json generated by process_txt() function
    
    json_data_paths = {
        'nodes': '',
        'proc_process': '',
        'proc_file': '',
        'proc_reg': '',
        'proc_network': ''
    }
    json_data = {
        'nodes': {},
        'proc_process': {},
        'proc_file': {},
        'proc_reg': {},
        'proc_network': {}
    }
    
    word_dict = []
    word_dict_node = []
    word_dict_edge = []

    current_edge_id = -1
    current_node_id = -1
    current_node_id_of_current_graph = -1
    api_nodes_existed_id = {}
    api_nodes_existed = {}

    # path_type_code = {
    #     'proc_reg': 0,
    #     'proc_file': 1,
    #     'proc_network': 2,
    #     'proc_process': 3
    # }
    # node_type_code = {
    #     'proc': 0, # process_handle
    #     'file': 1, # file_handle
    #     'reg': 2, # registry key_handle

    #     # 'network': 2,
    #     'process_api': 3,
    #     'file_api': 4,
    #     'reg_api': 5,
    # }
    path_type_code = {
        'proc_reg': 0,
        'proc_file': 1,
        'proc_process': 2
    }
    node_type_code = {
        'proc': 0, # process_handle
        'file': 1, # file_handle
        'reg': 2, # registry key_handle

        # 'network': 2,
        'process_api': 3,
        'file_api': 4,
        'reg_api': 5,
    }
    node_color = ['red', 'orange', 'blue', 'pink', 'yellow', 'cyan']

    interesting_apis = care_APIs() + list(node_type_code.keys()) + ['Other']
    # create_apis = ['Open', 'Create', 'Set', 'Write']

    handle_to_pid = {}
    handle_to_node = {}
    pid_to_node = {}

    data_dortmund_format = {}
    max_n_nodes = 0

    word_to_ix_node = {}
    word_to_ix_edge = {}
    word_to_ix = {}

    def __init__(self, reports_parent_dir_path=None, data_json_path=None, pickle_folder=None, vocab_path=None, encode_edge_data=True, save_json=True, use_interesting_apis=True, prepend_vocab=True, mapping_path=None):
        """
        - reports_parent_dir_path: folder contains child folders which represent categories of reports (each graph label is 1 folder). In each folder are report.json generated by cuckoo
        - data_json_path: path to data.json generated by process_txt() function
        """
        # self.vocab_path = vocab_path.split('.txt')[0]
        self.vocab_path = vocab_path
        # self.vocab_path = vocab_path
        self.vocab_path_node = self.vocab_path+'/node.txt'
        self.vocab_path_edge = self.vocab_path+'/edge.txt'
        
        self.encode_edge_data = encode_edge_data # encode edge data. If True, encode arguments / flags of each API. If False, encode type of API only
        self.save_json = True

        self.use_interesting_apis = use_interesting_apis
        self.prepend_vocab = prepend_vocab
        self.mapping_path = mapping_path
        print('self.prepend_vocab', self.prepend_vocab)
        print('self.use_interesting_apis', self.use_interesting_apis)

        if reports_parent_dir_path is not None:
            self.reports_parent_dir_path = reports_parent_dir_path

        if data_json_path is not None:
            self.final_json_path = data_json_path
            data_dir = os.path.dirname(self.final_json_path)
            if not os.path.isdir(data_dir):
                os.makedirs(data_dir)
                
            # json_data_dir = data_json_path.split('.json')[0]
            json_data_dir = data_json_path
            if not os.path.isdir(json_data_dir):
                os.makedirs(json_data_dir)
            for key in self.json_data_paths:
                self.json_data_paths[key] = json_data_dir+'/'+key+'.json'
                

        if pickle_folder is not None:
            self.pickle_folder = pickle_folder
            if not os.path.isdir(self.pickle_folder):
                os.makedirs(self.pickle_folder)

        return

    def load_data(self, from_folder=False, from_json=False, from_pickle=False):
        """
        Return self.data_dortmund_format
        """
        if from_pickle is True:
            print('Load data from pickle folder')
            self.load_from_pickle()
        else:
            ''' Copy prep_data file to this data folder '''
            if self.final_json_path is not None:
                shutil.copy('./utils/prep_data.py', self.final_json_path+'/../prep_data.py')

            if from_folder is True:
                if from_json is False:
                    self.save_json = True
                
                print(
                    '\nProcess data from reports folder to data.json and encode nodes & edges')
                self.encode_reports_from_dir(self.save_json)
                if from_json is True:
                    print('\nEncode nodes & edges')
                    self.encode_data()
            
            elif from_json is True:
                print('\nLoad data from json file')
                self.load_data_json()
                print('\nEncode nodes & edges')
                self.encode_data()

            print('\nCreate graphs and save to pickle files from encoded data')
            self.create_graphs()

        return self.data_dortmund_format

    def encode_reports_from_dir(self, write_to_json=True):
        for report_dir_name in os.listdir(self.reports_parent_dir_path):
            report_dir_path = os.path.join(
                self.reports_parent_dir_path, report_dir_name)
            for report_file_name in os.listdir(report_dir_path):
                # print('report_file_name', report_dir_name+'/'+report_file_name)
                behavior = self.read_report(os.path.join(
                    report_dir_path, report_file_name))

                if behavior is not None:
                    self.encode_report(
                        behavior, report_file_name, report_dir_name)
                else:
                    print('behavior none. Skip ' +
                          report_dir_name+'/'+report_file_name)
        if write_to_json is True:
            print('Process done. Saving to json file...')
            # Save json data to file
            # it's gonna be too large, use separate file to store information as belows
            for key in self.json_data_paths:
                with open(self.json_data_paths[key], 'w') as outfile:
                    json.dump(self.json_data[key], outfile)
        else:
            print('Writing to json file option set to False. Skip saving.')

    def read_report(self, report_file_path):
        print('report_file_path', report_file_path)
        with open(report_file_path) as json_file:
            data = json.load(json_file)
            if 'behavior' in data.keys():
                return data['behavior']
            else:
                print('No behavior tag found.')
                return None
            # self.behavior = data['behavior']

    def load_data_json(self):
        for key in self.json_data_paths:
                with open(self.json_data_paths[key]) as json_file:
                    self.json_data[key] = json.load(json_file)

    def encode_report(self, behavior, report_name, report_folder):
        """
        Process the data extracted from the report and save to data.json.
        Encode node, edge and save to graphs_dict.
        --------------------------------------
        Get and save the nodes, meta-path
            - report_folder: category of report (graph label)
            - report_name: graph name (graph id)
        Encode node and edge and save to graphs_dict also
        """

        # to debug single file
        # print('report_name', report_name)
        # if report_name != '8f5135eec4dcb808423209163bbd94025ec47f4cb1b20dcf75b1fd56773ac58f.json':
        #     return

        self.current_node_id_of_current_graph = -1
        graph_name = report_folder+'__'+report_name
        
        #####################
        # Get all the procs
        #####################
        # print(len(behavior['processes']))
        # print(behavior['processtree'])
        procs = behavior['processes']

        # print('=================================')
        # print(self.pid_to_node)
        # print('=================================\n')

        for proc in procs:
            calls = proc['calls']
            
            # id \t proc_name \t proc_path_severity \t regkey_written_severity \t dll_loaded_severity \t connects_host_severity
            # proc_name = proc['process_name']
            proc_name = proc['process_path']
            proc_info = '{}|{}'.format(graph_name, proc_name)

            if len(calls) > 0:  # this process does have api calls
                ##############################
                # Now loop through all the api calls
                ##############################
                for api in calls:
                    cat = api['category']
                    

                    # if cat in self.allow_cat and api['api'] in self.interesting_apis:
                    if cat in self.allow_cat:
                        __proc_identifier__ = graph_name+'_proc_'+str(proc['pid'])
                        # create parent node (root proc) if not inserted yet
                        if __proc_identifier__ not in self.pid_to_node:
                            self.increase_node()
                            proc_data = {
                                # 'name': proc_name.replace(' ', '^'),
                                'name': 'proc{'+str(proc['pid'])+'}',
                                'pid': proc['pid'],
                                'type': 'proc',
                                'id': self.current_node_id,
                                'id_in_graph': self.current_node_id_of_current_graph,
                                'graph': graph_name,
                                'graph_label': report_folder
                            }
                            # self.json_data['nodes'].append(proc_data)
                            self.json_data['nodes'][proc_data['id']] = proc_data
                            self.pid_to_node[graph_name+'_proc_'+str(proc['pid'])] = proc_data
                            
                        
                        # if api['api'] in self.interesting_apis:
                        #     api_name = api['api']
                        # else:
                        #     api_name = 'Other'
                        api_name = api['api']
                        
                        api_time = api['time']
                        api_info = '{}|{}'.format(graph_name, api_name)

                        # print(api)

                        if api_name in self.interesting_apis:
                            if cat == 'file': # process API type file
                                self.process_API_file(api, api_info, proc_data, graph_name, report_folder)

                            if cat == 'process': # process API type process
                                self.process_API_process(api, api_info, proc_data, graph_name, report_folder)

                            if cat == 'registry': # process API type registry
                                self.process_API_registry(api, api_info, proc_data, graph_name, report_folder)
                            
                
        # print('\tDone')
        return self.final_json_path
    
    def increase_node(self, api_info=None):
        self.current_node_id = self.current_node_id + 1
        self.current_node_id_of_current_graph = self.current_node_id_of_current_graph + 1

        self.api_nodes_existed_id[api_info] = self.current_node_id


    def process_API_process(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']
        api_flags = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']
        # api_args = 'NULL'
        # if 'arguments' in api and api['arguments'] is not None and api['arguments'] != '':
        #     api_args = getInterestingArg(api['arguments'])


        
        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'],
                'type': 'process_api',
                
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.api_nodes_existed_id[api_info] = self.current_node_id

        buffer_length = 0
        if 'buffer' in api['arguments'] and 'length' in api['arguments']:
            buffer_length = api['arguments']['length']

        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'process_handle' in api['arguments'] and 'process_identifier' in api['arguments'] and api['arguments']['process_identifier'] != 0:

            # create this process API node ONLY WHEN there is a reference from this API to the a process handle (process_handle != 0)
            # node process API data
            # self.increase_node()
            # node_api__data = {
            #     'name': api['api'],
            #     'type': 'process_api',
                    
            #     'id': self.current_node_id,
            #     'id_in_graph': self.current_node_id_of_current_graph,

            #     'graph': graph_name,
            #     'graph_label': graph_label
            # }
            # self.json_data['nodes'].append(node_api__data)
            # self.api_nodes_existed_id[api_info] = self.current_node_id


            process_identifier = str(api['arguments']['process_identifier'])
            # process_handle = api['arguments']['process_handle']

            # save this node to list pid_to_node first, in case later needs query
            # self.pid_to_node[graph_name+'_'+api['pid']] = node_api__data

            __identifier__ = graph_name+'_proc_'+process_identifier
            if __identifier__ not in self.pid_to_node:
                # if api_name == 'NtOpenProcess':
                # create a process node
                self.increase_node()
                # node file handle data
                node_process__data = {
                        'name': 'proc{'+str(process_identifier)+'}',
                        'type': 'proc',
                        
                        'id': self.current_node_id,
                        'id_in_graph': self.current_node_id_of_current_graph,

                        'graph': graph_name,
                        'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_process__data)
                self.json_data['nodes'][node_process__data['id']] = node_process__data

                self.pid_to_node[__identifier__] = node_process__data
                    
            # get the node to connect to
            connect_node = self.pid_to_node[__identifier__]
                                    
            # create an edge from parent_node to node_api if there the process_identifier is different from parent_node's pid
            if int(parent_node['pid']) != int(process_identifier):
                self.edge(parent_node, node_api__data, {'api_flags': api_flags, 'edge_type': 'proc_process'}, graph_name, buffer_size=buffer_length)
            else:
                # create edge between this node and connect_node
                if 'Open' in api_name or 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                    self.edge(node_api__data, connect_node, {'api_flags': api_flags, 'edge_type': 'proc_process'}, graph_name, buffer_size=buffer_length)
                else:
                    self.edge(connect_node, node_api__data, {'api_flags': api_flags, 'edge_type': 'proc_process'}, graph_name, buffer_size=buffer_length)
        
        # Actually we don't care about those API that do not reference process_identifier to any process_identifier, so just comment these
        # else:
        #     # create edge from this api to proc node (parent_node)
        #     # but because this is process, this edge is the same with the edge created above (from node_api__data to connect_node)
        #     self.edge(parent_node, node_api__data, {'api_flags': api_flags, 'edge_type': 'proc_process'}, graph_name, buffer_size=buffer_length)
                

    def process_API_file(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']

        api_flags = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']

        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'],
                'type': 'process_api',
                        
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.api_nodes_existed_id[api_info] = self.current_node_id

        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'file_handle' in api['arguments']:
            file_handle = api['arguments']['file_handle']

            __identifier__ = graph_name+'_file_'+file_handle
            # if api_name == 'NtCreateFile':
            if __identifier__ not in self.handle_to_node:
                # create a file node
                self.increase_node()
                # node file handle data
                node_file__data = {
                    'name': 'file{'+file_handle+'}',
                    'type': 'file',
                    
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_file__data)
                self.json_data['nodes'][node_file__data['id']] = node_file__data

                self.handle_to_node[__identifier__] = node_file__data
            
            connect_node = self.handle_to_node[__identifier__]

            buffer_length = 0
            if 'buffer' in api['arguments'] and 'length' in api['arguments']:
                buffer_length = api['arguments']['length']

            # create edge from this api node to connect_node (file handle) (file node)
            if 'Open' in api_name or 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                self.edge(node_api__data, connect_node, {'edge_type': 'proc_file'}, graph_name, buffer_size=buffer_length)
            else:
                self.edge(connect_node, node_api__data, {'edge_type': 'proc_file'}, graph_name, buffer_size=buffer_length)

        # create edge from this api to proc node (parent_node)
        # self.edge(parent_node, node_api__data, {'api_flags': api_flags, 'edge_type': 'proc_file'}, graph_name)



    def process_API_registry(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']

        api_flags = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']

        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'],
                'type': 'process_api',
                        
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]
        
            # print('node_api__data', node_api__data)

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.api_nodes_existed_id[api_info] = self.current_node_id

        # if this api has key_handle, then get the key_handle, then find the node correspond with this key_handle, then connect the api_node with the key_handle node
        if 'key_handle' in api['arguments']:
            key_handle = api['arguments']['key_handle']

            __identifier__ = graph_name+'_reg_'+key_handle
            # if api_name == 'NtOpenKey':
            if __identifier__ not in self.handle_to_node:
                # create a registry key node
                self.increase_node()
                # node key handle data
                node_reg__data = {
                    'name': 'reg{'+key_handle+'}',
                    'type': 'reg',
                    
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_reg__data)
                self.json_data['nodes'][node_reg__data['id']] = node_reg__data

                self.handle_to_node[__identifier__] = node_reg__data
            
            connect_node = self.handle_to_node[__identifier__]

            buffer_length = 0
            if 'buffer' in api['arguments'] and 'length' in api['arguments']:
                buffer_length = api['arguments']['length']

            # create edge from this api node to connect_node (file handle) (file node)
            if 'Open' in api_name or 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                self.edge(node_api__data, connect_node, {'edge_type': 'proc_reg'}, graph_name, buffer_size=buffer_length)
            else:
                self.edge(connect_node, node_api__data, {'edge_type': 'proc_reg'}, graph_name, buffer_size=buffer_length)

        # create edge from this api to proc node (parent_node)
        # self.edge(parent_node, node_api__data, {'api_flags': api_flags, 'edge_type': 'proc_reg'}, graph_name)



    def edge(self, s, d, args, graph_name, buffer_size=0):
        self.current_edge_id += 1

        # if buffer_size <= 0:
        #     return

        if buffer_size > 0:
            print('buffer', buffer_size)

        path_data = {
            # 'type': self.path_type_code[args['edge_type']],
            'type': args['edge_type'],
            'args': '',
            'from': s['id'],
            'to': d['id'],
            
            'from_in_graph': s['id_in_graph'],
            'to_in_graph': d['id_in_graph'],
            
            'id': self.current_edge_id,
            'buffer_size': buffer_size,

            'graph': graph_name
        }

        # care only when this api (s and d) is not Other (in interesting apis)
        if args is not None and 'api_flags' in args and self.nodename_to_viz(s['name']) != 'Other' and self.nodename_to_viz(d['name']) != 'Other':
            path_data['args'] = args['api_flags']
                        
        # self.json_data[args['edge_type']].append(path_data)
        self.json_data[args['edge_type']][path_data['id']] = path_data
        # self.json_data['path'][path_data['id']] = path_data



    def encode_node(self, node):
        """
        Encode node information to node attribute
        ----------------------------
            Calculate node attributes (init features)
            All nodes must have same features space.
            Declare a space whereas:
              - 5 first elements represent node type
                  (one-hot encoding for 5 types)

              - Next 10 elements represent the node name
                  (api name / proc name) (use word embedding)

              - Next 5 element represent the path_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the regkey_written_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the dll_loaded_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the connects_host_severity
                  for the "proc" node (one-hot encoding for 5 levels)

              - Next 5 element represent the args_severity
                  for the "api" node (one-hot encoding for 5 levels)
              - Next 5 element represent the flags_severity
                  for the "api" node (one-hot encoding for 5 levels)
        """
        # =======================================
        # Encode node name using Word Embedding
        # =======================================

        # Use name of API to represent each node
        self.nodes_labels = self.nodes_labels + [node['name']]

        ###################
        # Create graph
        ###################
        if node['graph'] not in self.graphs_dict.keys():
            self.graphs_name_to_label[node['graph']] = node['graph_label']
            self.graphs_dict[node['graph']] = DGLGraph(multigraph=True)
            self.graphs_viz[node['graph']] = Digraph(
                name=node['graph_label'], format='png')

        ###################
        # Get features
        ###################

        ndata = {}

        ''' GNN_NODE_TYPES_KEY '''
        # print('node_type_encoded', node_type_encoded)
        '''use this with node_one_hot option = True in config'''
        # nte_torch = torch.from_numpy(np.expand_dims(np.array(self.node_type_code[node['type']]), axis=0))
        '''use this with node_one_hot option = False in config'''
        node_type_encoded = indices_to_one_hot(
            self.node_type_code[node['type']], out_vec_size=len(self.node_type_code))
        nte_torch = torch.from_numpy(np.array([node_type_encoded]))
        nte_torch = torch.tensor(nte_torch).type(torch.FloatTensor)
        # print('nte_torch', nte_torch)
        ndata[GNN_NODE_TYPES_KEY] = nte_torch

        ''' GNN_NODE_LABELS_KEY '''
        cbow_node = self.cbow_encode_node_name(self.nodename_to_str(node['name']))
        # print('cbow_node', cbow_node)
        ndata[GNN_NODE_LABELS_KEY] = cbow_node.view(1, -1)
        # node_embed = self.embedding(cbow_node)
        # # node_embed = self.embedding_node(cbow_node)
        # ndata[GNN_NODE_LABELS_KEY] = node_embed.view(1, -1)

        ''' save id of this node to read and calculating embedding by reading labels from file later when training or testing '''
        # if node['id'] == -1 or node['id'] == '-1':
        #     print("node['id'] == -1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        # ndata[GNN_NODE_LABELS_KEY] = torch.Tensor([[node['id']]])


        # print('ndata[GNN_NODE_LABELS_KEY]', ndata[GNN_NODE_LABELS_KEY])
        # print('ndata[GNN_NODE_TYPES_KEY]', ndata[GNN_NODE_TYPES_KEY])

        ''' GNN_NODE_ATTS_KEY '''
        # node_attr = np.concatenate([
        #     # [node['id']],
        #     [node['id_in_graph']],
        #     # node__type__encoded,
        #     [node['name']],
        #     [node['graph']],
        #     [node['graph_label']]
        # ], axis=0)
        # features = node_attr[1:-2]
        # print('features', features)
        # # features = sp.csr_matrix(features, dtype=np.float32)
        # # # features = preprocess_features(features) # sparse
        # # features = features.todense()
        # features_torch = torch.from_numpy(features)
        # features_torch = torch.tensor(
        #     features_torch).type(torch.FloatTensor)
        # # print('features_torch.shape', features_torch.shape)
        # # ndata[GNN_NODE_ATTS_KEY] = features_torch

        ''' add node with data to graph '''
        self.graphs_dict[node['graph']].add_nodes(1, data=ndata)

        # for visualize
        if node['type'] in ['proc', 'file', 'reg']:
            shape = 'ellipse'
        else:
            shape = 'box' # api node
        self.graphs_viz[node['graph']].node(
            'n{}'.format(node['id_in_graph']), self.nodename_to_viz(node['name']), color=self.node_color[self.node_type_code[node['type']]], shape=shape)

    def encode_edge(self, path):
        """
        Encode edge information to node attribute
        """

        if len(path) <= 0:
            del self.graphs_name_to_label[path['graph']]
            del self.graphs_dict[path['graph']]
            del self.graphs_viz[path['graph']]
            return

        self.edges_labels.append(self.path_type_code[path['type']])
        # self.edges_labels.append(path['type_code'])

        edata = {}

        ''' GNN_EDGE_TYPES_KEY '''
        '''use this with edge_one_hot = True in config'''
        # ete_torch = torch.from_numpy(np.expand_dims(np.array(self.path_type_code[path['type']]), axis=0))
        '''use this with edge_one_hot = False in config'''
        edge_type_encoded = indices_to_one_hot(
            self.path_type_code[path['type']], out_vec_size=len(self.path_type_code))
        ete_torch = torch.from_numpy(
            np.array([edge_type_encoded])).type(torch.FloatTensor)
        ete_torch = torch.tensor(ete_torch)
        # print('ete_torch', ete_torch)
        edata[GNN_EDGE_TYPES_KEY] = ete_torch

        if self.encode_edge_data is True:
            ''' GNN_EDGE_LABELS_KEY '''            
            cbow_edge = self.cbow_encode(args_to_str(path['args']))
            # print('args_to_str', args_to_str(path['args']))
            # print('cbow_edge', cbow_edge)
            # print('cbow_edge.shape', cbow_edge.shape)
            # print('edata[GNN_EDGE_TYPES_KEY].shape', edata[GNN_EDGE_TYPES_KEY].shape)
            edata[GNN_EDGE_LABELS_KEY] = cbow_edge.view(1, -1)
            # edge_embed = self.embedding(cbow_edge)
            # # edge_embed = self.embedding_edge(cbow_edge)
            # edata[GNN_EDGE_LABELS_KEY] = edge_embed.view(1, -1)
            
            ''' save id of this node to read and calculating embedding by reading labels from file later when training or testing '''
            # edata[GNN_EDGE_LABELS_KEY] = torch.Tensor([[path['id'], 0]])

            # print('edata[GNN_EDGE_LABELS_KEY]', edata[GNN_EDGE_LABELS_KEY])
            # print('edata[GNN_EDGE_TYPES_KEY]', edata[GNN_EDGE_TYPES_KEY])


            ''' GNN_EDGE_BUFFER_SIZE_KEY '''
            edata[GNN_EDGE_BUFFER_SIZE_KEY] = torch.Tensor([[path['buffer_size']]])
            # print('edata[GNN_EDGE_LABELS_KEY]', edata[GNN_EDGE_LABELS_KEY])
            # print('edata[GNN_EDGE_BUFFER_SIZE_KEY]', edata[GNN_EDGE_BUFFER_SIZE_KEY])
            # print(path['buffer_size'])
            

        ''' add edge with data to graph '''
        # print("path['graph']", path['graph'])
        # print(self.graphs_dict[path['graph']].number_of_nodes())
        self.graphs_dict[path['graph']].add_edge(
            path['from_in_graph'], path['to_in_graph'], data=edata)
        self.graphs_viz[path['graph']].edge('n{}'.format(
            path['from_in_graph']), 'n{}'.format(path['to_in_graph']))

    def encode_data(self):
        """
        Encode nodes & edges from data.json
        """
        # first create dictionary
        is_new_dict_node = False
        is_new_dict_edge = False
        if not os.path.isdir(self.vocab_path):
            os.makedirs(self.vocab_path)
        if not os.path.exists(self.vocab_path_node):
            self.create_dict_node()
            is_new_dict_node = True
            self.prepend_vocab = True
        if not os.path.exists(self.vocab_path_edge):
            self.create_dict_edge()
            is_new_dict_edge = True
            self.prepend_vocab = True
        # read from dict node
        with open(self.vocab_path_node, 'r') as f:
            vocab = f.read().strip()
            self.word_dict_node = vocab.split(' ')
            self.append_dict_node()
            # vocab_size = len(vocab)
            # print('vocab size: {}'.format(vocab_size))
            self.word_to_ix_node = {word: i for i,
                               word in enumerate(self.word_dict_node)}
        # read from dict edge
        with open(self.vocab_path_edge, 'r') as f:
            vocab = f.read().strip()
            self.word_dict_edge = vocab.split(' ')
            self.append_dict_edge()
            # vocab_size = len(vocab)
            # print('vocab size: {}'.format(vocab_size))
            self.word_to_ix_edge = {word: i for i,
                               word in enumerate(self.word_dict_edge)}

        # num_token_node = len(self.word_dict_node)
        # self.embedding_node = nn.Embedding(num_token_node, 1)
        # num_token_edge = len(self.word_dict_edge)
        # self.embedding_edge = nn.Embedding(num_token_edge, 1)

        self.word_dict = self.word_dict_node + self.word_dict_edge
        self.word_to_ix = {word: i for i,
                               word in enumerate(self.word_dict)}
        self.num_token = len(self.word_dict)
        # num_token = len(self.word_dict_node) + len(self.word_dict_edge)
        # self.embedding = nn.Embedding(num_token, 1)

        if 'nodes' in self.json_data.keys():
            n_num = 0
            n_tot = len(self.json_data['nodes'])
            # self.embed_nodes = nn.Embedding(n_tot, 1)
            print('\nencode_node')
            for node_id in self.json_data['nodes']:
                node = self.json_data['nodes'][node_id]
                # print('encode_node ', node)
                self.encode_node(node)
                n_num += 1
                if n_num % 100000 == 0 or n_num == n_tot:
                    print('{}/{}'.format(n_num, n_tot))
        
        # if 'path' in self.json_data.keys():
        #     p_num = 0
        #     p_tot = len(self.json_data['path'])
        #     # self.embed_edges = nn.Embedding(p_tot, 1)
        #     print('\nencode_edge')
        #     for path_id in self.json_data['path']:
        #         path = self.json_data['path'][path_id]
        #         # print('encode_edge ', path)
        #         self.encode_edge(path)
        #         p_num += 1
        #         if p_num % 100000 == 0 or p_num == p_tot:
        #             print('{}/{}'.format(p_num, p_tot))
        for key in self.json_data:
            if key != 'nodes':
                p_num = 0
                p_tot = len(self.json_data[key])
                # self.embed_edges = nn.Embedding(p_tot, 1)
                print('\nencode_edge type '+key)
                for path_id in self.json_data[key]:
                    path = self.json_data[key][path_id]
                    # print('encode_edge ', path)
                    self.encode_edge(path)
                    p_num += 1
                    if p_num % 100000 == 0 or p_num == p_tot:
                        print('{}/{}'.format(p_num, p_tot))

    def create_graphs(self):
        """
        Create graphs from encoded data to feed to network
        """
        # print(self.graphs_name_to_label)
        print('len(self.graphs_dict)', len(self.graphs_dict))
        ##############################
        # Append to graphs list
        ##############################
        gnum = 0
        for g_name in list(self.graphs_name_to_label.keys()):
            g_label = self.graphs_name_to_label[g_name]
            graph = self.graphs_dict[g_name]

            if not graph.edata:
                del self.graphs_name_to_label[g_name]
                del self.graphs_dict[g_name]

            else:
                n_nodes = graph.number_of_nodes()
                if n_nodes > self.max_n_nodes:
                    self.max_n_nodes = n_nodes

                ######################
                # Normalize edge
                ######################
                # edge_src, edge_dst = graph.edges()

                # edge_dst = list(edge_dst.data.numpy())
                # print('graph.edata ('+g_label+')', graph.edata)
                # # edge_type = list(graph.edata[GNN_EDGE_TYPES_KEY])
                # edge_lbl = list(graph.edata[GNN_EDGE_LABELS_KEY])

                # # print('edge_dst, edge_type', edge_dst, edge_type)
                # # _, inverse_index, count = np.unique((edge_dst, edge_type), axis=1, return_inverse=True, return_counts=True)
                # _, inverse_index, count = np.unique((edge_dst, edge_lbl), axis=1, return_inverse=True, return_counts=True)
                # degrees = count[inverse_index]
                # edge_norm = np.ones(
                #     len(edge_dst), dtype=np.float32) / degrees.astype(np.float32)
                # graph.edata[GNN_EDGE_NORM] = torch.FloatTensor(edge_norm)

                self.graphs.append(graph)
                self.graphs_names.append(g_name)
                self.graphs_labels.append(g_label)

                # Save this graph to png
                # if gnum < 10:
                if False:
                    # print(graph)
                    # nx.draw(graph.to_networkx(), with_labels=True)
                    # plt.savefig('data/graphs/{}.png'.format(g_name))
                    # print(self.graphs_viz[g_name].source)
                    self.graphs_viz[g_name].render(
                        filename='data/graphviz/{}/{}'.format(os.path.basename(self.reports_parent_dir_path), g_name))
                gnum += 1

        # print(self.graphs)

        if self.mapping_path is not None:
            with open(self.mapping_path) as json_file:
                mapping = json.load(json_file)
            with open(self.pickle_folder+'/../mapping.json', 'w') as f:
                json.dump(mapping, f)
        else:
            label_set = set(sorted(self.graphs_labels))  # malware: 0, benign: 1
            num_labels = len(label_set)
            mapping = dict(zip(label_set, list(range(num_labels))))
            with open(self.pickle_folder+'/../mapping.json', 'w') as f:
                json.dump(mapping, f)
        
        # mapping = self.mapping_labels
        num_labels = len(mapping)
        print('num_labels', num_labels)
        print('mapping', mapping)
        labels = [mapping[label] for label in self.graphs_labels]
        # print('labels', labels)
        # print('label_set', label_set)

        num_entities = len(set(self.nodes_labels))
        num_rels = len(set(self.edges_labels))

        labels_torch = torch.LongTensor(labels)
        print('labels_torch', labels_torch)

        torch.save(labels_torch, os.path.join(self.pickle_folder, LABELS))
        save_pickle(num_labels, os.path.join(self.pickle_folder, N_CLASSES))
        save_pickle(num_entities, os.path.join(self.pickle_folder, N_ENTITIES))
        save_pickle(num_rels, os.path.join(self.pickle_folder, N_RELS))
        save_pickle(self.graphs, os.path.join(self.pickle_folder, GRAPH))
        save_pickle(self.graphs_names, os.path.join(self.pickle_folder, GNAMES))
        save_pickle(self.max_n_nodes, os.path.join(
            self.pickle_folder, MAX_N_NODES))
        save_pickle(self.graphs_labels, os.path.join(
            self.pickle_folder, LABELS_TXT))
        save_txt(self.graphs_names, os.path.join(self.pickle_folder, GNAMES+'.txt'))

        self.data_dortmund_format = {
            GRAPH: self.graphs,
            GNAMES: self.graphs_names,
            N_CLASSES: num_labels,
            N_ENTITIES: num_entities,
            N_RELS: num_rels,
            LABELS: labels_torch,
            MAX_N_NODES: self.max_n_nodes,
            LABELS_TXT: self.graphs_labels
        }

    def load_from_pickle(self):
        """
        Load data from pickle files
        """
        print(os.path.join(self.pickle_folder, GRAPH))
        self.data_dortmund_format = {
            GRAPH: load_pickle(os.path.join(self.pickle_folder, GRAPH)),
            GNAMES: load_pickle(os.path.join(self.pickle_folder, GNAMES)),
            N_CLASSES: load_pickle(os.path.join(self.pickle_folder, N_CLASSES)),
            N_ENTITIES: load_pickle(os.path.join(self.pickle_folder, N_ENTITIES)),
            N_RELS: load_pickle(os.path.join(self.pickle_folder, N_RELS)),
            LABELS: torch.load(os.path.join(self.pickle_folder, LABELS)),
            MAX_N_NODES: load_pickle(os.path.join(self.pickle_folder, MAX_N_NODES)),
            LABELS_TXT: load_pickle(os.path.join(self.pickle_folder, LABELS_TXT)),
            # GRAPH_ADJ: torch.load(os.path.join(self.pickle_folder, GRAPH_ADJ))
        }

        return self.data_dortmund_format

    def create_dict_node(self):
        """
        Create dictionary of name and arguments to encode
        """
        print('create_dict_node')
        self.word_dict_node.append('Other')
        self.append_dict_node()
    
    def create_dict_edge(self):
        """
        Create dictionary of name and arguments to encode
        """
        print('create_dict_edge')
        self.word_dict_edge.append('NULL')
        self.append_dict_edge()
    
    
    def append_dict_node(self):
        ''' use vocab for nodes separately '''
        if 'nodes' in self.json_data.keys():
            nlen = len(self.json_data['nodes'])
            print('nlen', nlen)
            ncount = 0
            for node_id in self.json_data['nodes']:
                node = self.json_data['nodes'][node_id]
                ncount += 1
                if ncount % 10000 == 0 or ncount == nlen:
                    print('update vocab at node {}/{}'.format(ncount, nlen))
                node_name = self.nodename_to_str(node['name'])
                if node_name not in self.word_dict_node:
                    print('\t'+node['name']+' not in word_dict_node', self.prepend_vocab)
                    if self.prepend_vocab is True:
                        self.word_dict_node.append(node_name)
                    else:
                        print('\t\tChange node name', node_id, self.json_data['nodes'][node_id]['name'])
                        self.json_data['nodes'][node_id]['name'] = 'Other'
        ''' save vocab '''
        if self.prepend_vocab is True:
            print('save vocab')
            with open(self.vocab_path_node, 'w') as f:
                f.write(' '.join(self.word_dict_node))

        if self.prepend_vocab is False:
            print('Change json file nodes')
            with open(self.json_data_paths['nodes'], 'w') as f:
                json.dump(self.json_data['nodes'], f)

    def append_dict_edge(self):
        if self.encode_edge_data is True:
            for key in self.json_data:
                if key != 'nodes':
                    plen = len(self.json_data[key])
                    pcount = 0
                    for path_id in self.json_data[key]:
                        path = self.json_data[key][path_id]
                        pcount += 1
                        if pcount % 10000 == 0 or pcount == plen:
                            print('update vocab at edge type {} {}/{}'.format(key, pcount, plen))
                        # print('path', path)
                        txt = args_to_str(path['args'])
                        # print('txt', txt)
                        for word in txt.split(' '):
                            if len(word) > 0 and word != ' ' and word not in self.word_dict_edge:
                                print('\t['+word+'] not in word_dict_edge')
                                # self.word_dict_edge.append(word)

                                if self.prepend_vocab is True:
                                    self.word_dict_edge.append(word)
                                else:
                                    print('\t\tChange edge args', path_id, self.json_data[key][path_id]['args'])
                                    txt = txt.replace(word, 'NULL')
                                    self.json_data[key][path_id]['args'] = txt

        ''' save vocab '''
        if self.prepend_vocab is True:
            print('save vocab')
            with open(self.vocab_path_edge, 'w') as f:
                f.write(' '.join(self.word_dict_edge))

        if self.prepend_vocab is False:
            for key in self.json_data_paths:
                if key != 'nodes':
                    print('Change json file ', key)
                    with open(self.json_data_paths[key], 'w') as f:
                        json.dump(self.json_data[key], f)

    def cbow_encode_node_name(self, raw_text):
        # if self.use_interesting_apis:
        #     word_to_ix = {word: i for i,
        #                         word in enumerate(self.interesting_apis)}
        #     idxs = [word_to_ix[raw_text]]
        #     return torch.tensor(idxs)
        # else:
        #     data = []
        #     # target = raw_text
        #     data.append(raw_text)
        #     return make_vector(data, self.word_to_ix)
        data = []
        # target = raw_text
        data.append(raw_text)
        return make_vector(data, self.word_to_ix)
        # return make_vector(data, self.word_to_ix_node)

    def cbow_encode(self, raw_text):
        if len(raw_text) == 0:
            raw_text = 'NULL NULL'
        raw_text = raw_text.split(' ')
        # print('\t raw_text', raw_text)
        data = []
        i = 1
        while i < len(raw_text):
            # context = [raw_text[i - 2], raw_text[i - 1],
            #            raw_text[i + 1], raw_text[i + 2]]
            target = raw_text[i]
            context = [raw_text[i - 1], target]
            data.append((context, target))
            # print('i', i, 'context', context)
            i += 2
        # print(data)
        # print(data[0])
        # print(data[0][0])
        # print('\t data[:5]', data[:5])
        # print('make_context_vector(data[0][0], self.word_to_ix)', make_context_vector(data[0][0], self.word_to_ix))
        return make_context_vector(data[0][0], self.word_to_ix)
        # return make_context_vector(data[0][0], self.word_to_ix_edge)


    def nodename_to_viz(self, txt):
        # print('self.use_interesting_apis', self.use_interesting_apis)
        if self.use_interesting_apis is False:
            return txt
        
        if txt.split('{')[0] not in self.interesting_apis:
            txt = 'Other{'+txt+'}'
        return txt

    def nodename_to_str(self, txt):
        if self.use_interesting_apis is False:
            return txt.split('{')[0]

        if txt.split('{')[0] not in self.interesting_apis:
            return 'Other'

        return txt.split('{')[0]


class CBOW(nn.Module):

    def __init__(self):
        pass

    def forward(self, inputs):
        pass


def make_vector(words, word_to_ix):
    idxs = [word_to_ix[w] for w in words]
    return torch.tensor(idxs)
    # return torch.tensor([idxs])


def make_context_vector(context, word_to_ix):
    # print('context', context)
    idxs = []
    for w in context:
        if len(w) == 0: 
            w = 'NULL'
        idxs.append(word_to_ix[w])
    # idxs = [word_to_ix[w] for w in context]
    return torch.tensor(idxs)
    # return torch.tensor([idxs])


def args_to_str(args_):
    str_ = str(args_)
    str_ = str_.replace('{', '').replace('}', '').replace('\'', '').replace(
        '"', '').replace(':', ' ').replace(',', ' ').replace('|', ' ').replace('  ', ' ')
    return str_


