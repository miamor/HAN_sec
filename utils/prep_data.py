"""
Prepare data
From report.json create:
    3 files for meta_path:
        proc_file.txt
        proc_net.txt
        proc_reg.txt
        proc_procapi.txt
    3 files for objects (nodes):
        proc.txt
        file.txt
        reg.txt

+ Meta_path data file format:
    n lines for n paths of this type in graph, each line:
        id_of_object_type_i \t id_of_object_type_j

+ proc object data file format:
    n lines for n nodes:
        id \t proc_name \t proc_path_severity \t regkey_written_severity \t dll_loaded_severity \t connects_host_severity \t

+ reg/net/file/procapi object data file format:
    - Show details of the api that links to it,
    - If n api falls into category registry/network/file/process, then there will be n reg/net/file/procapi nodes, each represents the characteristics of corresponding api.
        * note that procapi object is different from proc object.
    - If one api (with the same characteristics after calculating) is called multiple times, insert it once into the nodelist, later in the meta_path file links to the existed node's id.
        id \t api_name \t time \t arguments_severity \t flags_severity
            reg args eg:
                        "arguments": {
                            "key_handle": "0x00000070",
                            "desired_access": "0x00020019",
                            "regkey": "HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Nls\\ExtendedLocale"
                        },
            proc args eg:
                        "arguments": {
                            "section_handle": "0x00000074",
                            "object_handle": "0x00000000",
                            "desired_access": "0x000f0005",
                            "protection": 2,
                            "section_name": "",
                            "file_handle": "0x00000070"
                        },
            file args eg:
                        "arguments": {
                            "create_disposition": 1,
                            "file_handle": "0x00000070",
                            "filepath": "C:\\Windows\\Globalization\\Sorting\\sortdefault.nls",
                            "desired_access": "0x80100080",
                            "file_attributes": 128,
                            "filepath_r": "\\??\\C:\\Windows\\Globalization\\Sorting\\sortdefault.nls",
                            "create_options": 96,
                            "status_info": 1,
                            "share_access": 1
                        },
"""

import os
import json
import numpy as np
import scipy.sparse as sp
from .utils import indices_to_one_hot, label_encode_onehot, sample_mask, preprocess_adj, preprocess_features, save_pickle, load_pickle
from dgl import DGLGraph
import math
import random
from utils.constants import *

import torch
import torch.nn as nn

from graphviz import Digraph
import networkx as nx
import matplotlib.pyplot as plt


class PrepareData(object):
    DATA_OUT_PATH = ''  # data root dir
    # folder contains pickle file (save data in dortmund format)
    pickle_folder = ''

    reports_parent_dir_path = ''  # path to report.json generated by cuckoo
    behavior = {}  # behavior extracted from report.json in json format
    # consider only apis that fall into these categories
    # allow_cat = ['network', 'file', 'registry', 'process']
    allow_cat = ['file', 'process', 'registry']

    nb_attributes = 100  # size of feature space
    edges = None
    nodes_labels = []
    edges_labels = []
    graphs_labels = []

    graphs_dict = dict()
    graphs_name_to_label = dict()
    graphs = []
    graphs_viz = dict()

    final_json_data = {
        'nodes': [],
        'paths': []
    }
    final_json_path = ''  # path to data.json generated by process_txt() function
    
    json_data_paths = {
        'nodes': '',
        'proc_process': '',
        'proc_file': '',
        'proc_registry': '',
        'proc_network': ''
    }
    json_data = {
        'nodes': [],
        'proc_process': [],
        'proc_file': [],
        'proc_registry': [],
        'proc_network': []
    }
    
    word_dict = []

    current_node_id = -1
    current_node_id_of_current_graph = -1
    nodes_existed_id = {}

    path_type_code = {
        'proc_registry': 0,
        'proc_file': 1,
        'proc_network': 2,
        'proc_process': 3
    }
    node_type_code = {
        'proc': 0, # process_handle
        'file': 1, # file_handle
        'reg': 2, # registry key_handle

        # 'network': 2,
        'process_api': 3,
        'file_api': 4,
        'reg_api': 5,
    }
    
    handle_to_pid = {}
    handle_to_node = {}
    pid_to_node = {}

    data_dortmund_format = {}
    max_n_nodes = 0

    word_to_ix = {}

    def __init__(self, reports_parent_dir_path=None, data_json_path='data/data.json', pickle_folder='data/pickle', vocab_path='data/vocab.txt', encode_edge_data=True, save_json=True):
        """
        - reports_parent_dir_path: folder contains child folders which represent categories of reports (each graph label is 1 folder). In each folder are report.json generated by cuckoo
        - data_json_path: path to data.json generated by process_txt() function
        """
        self.vocab_path = vocab_path
        self.encode_edge_data = False # encode edge data. If True, encode arguments / flags of each API. If False, encode type of API only
        self.save_json = True

        if reports_parent_dir_path is not None:
            self.reports_parent_dir_path = reports_parent_dir_path

        if data_json_path is not None:
            self.final_json_path = data_json_path
            data_dir = os.path.dirname(self.final_json_path)
            if not os.path.isdir(data_dir):
                os.makedirs(data_dir)
                
            json_data_dir = data_json_path.split('.json')[0]
            if not os.path.isdir(json_data_dir):
                os.makedirs(json_data_dir)
            for key in self.json_data_paths:
                self.json_data_paths[key] = json_data_dir+'/'+key+'.json'


        if pickle_folder is not None:
            self.pickle_folder = pickle_folder
            if not os.path.isdir(self.pickle_folder):
                os.makedirs(self.pickle_folder)

        return

    def load_data(self, from_pickle=False, from_folder=False, from_json=False):
        """
        Return self.data_dortmund_format
        """
        if from_pickle is True:
            print('Load data from pickle folder')
            self.load_from_pickle()
        else:
            if from_folder is True:
                if from_json is False:
                    self.save_json = True
                
                print(
                    '\nProcess data from reports folder to data.json and encode nodes & edges')
                self.encode_reports_from_dir(self.save_json)
                if from_json is True:
                    print('\nEncode nodes & edges')
                    self.encode_data()
            
            elif from_json is True:
                print('\nLoad data from json file')
                self.load_data_json()
                print('\nEncode nodes & edges')
                self.encode_data()

            print('\nCreate graphs and save to pickle files from encoded data')
            self.create_graphs()

        return self.data_dortmund_format

    def encode_reports_from_dir(self, write_to_json=True):
        for report_dir_name in os.listdir(self.reports_parent_dir_path):
            report_dir_path = os.path.join(
                self.reports_parent_dir_path, report_dir_name)
            for report_file_name in os.listdir(report_dir_path):
                print('report_file_name', report_dir_name+'/'+report_file_name)
                behavior = self.read_report(os.path.join(
                    report_dir_path, report_file_name))

                if behavior is not None:
                    self.encode_report(
                        behavior, report_file_name, report_dir_name)
                else:
                    print('behavior none. Skip ' +
                          report_dir_name+'/'+report_file_name)
        if write_to_json is True:
            print('Process done. Saving to json file...')
            # Save json data to file
            # it's gonna be too large, use separate file to store information as belows
            for key in self.json_data_paths:
                with open(self.json_data_paths[key], 'w') as outfile:
                    json.dump(self.json_data[key], outfile)
        else:
            print('Writing to json file option set to False. Skip saving.')

    def read_report(self, report_file_path):
        with open(report_file_path) as json_file:
            data = json.load(json_file)
            if 'behavior' in data.keys():
                return data['behavior']
            else:
                print('No behavior tag found.')
                return None
            # self.behavior = data['behavior']

    def load_data_json(self):
        for key in self.json_data_paths:
                with open(self.json_data_paths[key]) as json_file:
                    self.json_data[key] = json.load(json_file)

    def encode_report(self, behavior, report_name, report_folder):
        """
        Process the data extracted from the report and save to data.json.
        Encode node, edge and save to graphs_dict.
        --------------------------------------
        Get and save the nodes, meta-path
            - report_folder: category of report (graph label)
            - report_name: graph name (graph id)
        Encode node and edge and save to graphs_dict also
        """

        # to debug single file
        # if report_name != '5c55a2608af9e496480021021ac4877b7742c025355475f02a8aca7fea684f8d.json':
        #     return

        print('process behavior from report ' +
              report_folder+'/'+report_name+' to txt')

        self.current_node_id_of_current_graph = -1
        graph_name = report_folder+'__'+report_name
        
        #####################
        # Get all the procs
        #####################
        # print(len(behavior['processes']))
        # print(behavior['processtree'])
        procs = behavior['processes']
        for proc in procs:
            calls = proc['calls']

            if len(calls) > 0:  # this process does have api calls
                self.current_node_id = self.current_node_id + 1
                self.current_node_id_of_current_graph = self.current_node_id_of_current_graph + 1

                # id \t proc_name \t proc_path_severity \t regkey_written_severity \t dll_loaded_severity \t connects_host_severity
                # proc_name = proc['process_name']
                proc_name = proc['process_path']

                # Save to final json data
                proc_data = {
                    # 'name': proc_name.replace(' ', '^'),
                    'name': 'proc',
                    'pid': proc['pid'],
                    'type': 'proc',
                    # 'type_code': self.node_type_code['proc'],
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,
                    'graph': graph_name,
                    'graph_label': report_folder
                }
                self.json_data['nodes'].append(proc_data)
                proc_info = '{}|{}'.format(graph_name, proc_name)
                self.nodes_existed_id[proc_info] = self.current_node_id
                self.pid_to_node[graph_name+'_'+str(proc['pid'])] = proc_data
                self.pid_to_node[graph_name+'_'+str(proc['ppid'])] = proc_data
                # self.encode_node(proc_data) # encode node

                # print(proc_data)

        # print('=================================')
        # print(self.pid_to_node)
        # print('=================================\n')

        for proc in procs:
            calls = proc['calls']
            
            if len(calls) > 0:  # this process does have api calls
                ##############################
                # Now loop through all the api calls
                ##############################
                for api in calls:
                    cat = api['category']

                    if cat in self.allow_cat:
                        api_name = api['api']
                        api_time = api['time']
                        api_info = '{}|{}'.format(graph_name, api_name)

                        # Check if this api (with the same characteristics (use name only as characteristics)) is called
                        if api_info not in self.nodes_existed_id.keys():
                            # print(api)

                            if cat == 'file': # process API type file
                                self.process_API_file(api, api_info, proc_data, graph_name, report_folder)

                            if cat == 'process': # process API type process
                                self.process_API_process(api, api_info, proc_data, graph_name, report_folder)

                            if cat == 'registry': # process API type registry
                                self.process_API_registry(api, api_info, proc_data, graph_name, report_folder)
                            
                
        print('\tDone')
        return self.final_json_path
    
    def increase_node(self, api_info=None):
        self.current_node_id = self.current_node_id + 1
        self.current_node_id_of_current_graph = self.current_node_id_of_current_graph + 1

        self.nodes_existed_id[api_info] = self.current_node_id


    def process_API_process(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']
        api_arg = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_arg = api['flags']


        # node_api__data = {
        #         'name': api['api'],
        #         'type': 'process_api',
        #                 # 'type_code': self.node_type_code['process_api'],
        #                 'id': self.current_node_id,
        #                 'id_in_graph': self.current_node_id_of_current_graph,

        #         'graph': graph_name,
        #         'graph_label': graph_label
        # }
        # self.json_data['nodes'].append(node_api__data)
        # self.nodes_existed_id[api_info] = self.current_node_id

        
        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'process_handle' in api['arguments'] and 'process_identifier' in api['arguments'] and api['arguments']['process_identifier'] != 0:

            # create this process API node ONLY WHEN there is a reference from this API to the a process handle (process_handle != 0)
            # node process API data
            self.increase_node()
            node_api__data = {
                'name': api['api'],
                'type': 'process_api',
                        # 'type_code': self.node_type_code['process_api'],
                        'id': self.current_node_id,
                        'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.json_data['nodes'].append(node_api__data)
            self.nodes_existed_id[api_info] = self.current_node_id


            process_identifier = str(api['arguments']['process_identifier'])
            process_handle = api['arguments']['process_handle']

            # save this node to list pid_to_node first, in case later needs query
            # self.pid_to_node[graph_name+'_'+api['pid']] = node_api__data

            if graph_name+'_'+process_handle not in self.handle_to_node:
                if graph_name+'_'+process_identifier in self.pid_to_node:
                    # self.handle_to_pid[process_handle] = process_identifier
                    # print('\n\t self.pid_to_node', self.pid_to_node)
                    self.handle_to_node[graph_name+'_'+process_handle] = self.pid_to_node[graph_name+'_'+process_identifier]
                else:
                    # if api_name == 'NtOpenProcess':
                    # create a process node
                    self.increase_node()
                    # node file handle data
                    node_process__data = {
                            'name': 'proc '+str(process_identifier),
                            'type': 'proc',
                                # 'type_code': self.node_type_code['proc'],
                                'id': self.current_node_id,
                                'id_in_graph': self.current_node_id_of_current_graph,

                            'graph': graph_name,
                            'graph_label': graph_label,
                    }
                    self.json_data['nodes'].append(node_process__data)
                    self.nodes_existed_id[api_info] = self.current_node_id

                    self.handle_to_node[graph_name+'_'+process_handle] = node_process__data
                    self.pid_to_node[graph_name+'_'+process_identifier] = node_process__data
                    
            # get the node to connect to
            connect_node = self.handle_to_node[graph_name+'_'+process_handle]
            
            # create edge between this node and connect_node
            if 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                self.edge(connect_node, node_api__data, {'api_arg': api_arg, 'edge_type': 'proc_process'}, graph_name)
            else:
                self.edge(node_api__data, connect_node, {'api_arg': api_arg, 'edge_type': 'proc_process'}, graph_name)
        
        # Actually we don't care about those API that do not reference process_identifier to any process_identifier, so just comment these 
        # else:
        #     # create edge from this api to proc node (parent_node)
        #     # but because this is process, this edge is the same with the edge created above (from node_api__data to connect_node)
        #     self.edge(parent_node, node_api__data, {'api_arg': api_arg, 'edge_type': 'proc_process'}, graph_name)
                

    def process_API_file(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']

        api_arg = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_arg = api['flags']

        # node file_api data
        self.increase_node()
        node_api__data = {
            'name': api['api'],
            'type': 'file_api',
                    # 'type_code': self.node_type_code['file_api'],
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

            'graph': graph_name,
            'graph_label': graph_label,
        }
        self.json_data['nodes'].append(node_api__data)
        self.nodes_existed_id[api_info] = self.current_node_id

        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'file_handle' in api['arguments']:
            file_handle = api['arguments']['file_handle']

            # if api_name == 'NtCreateFile':
            if file_handle not in self.handle_to_node:
                # create a file node
                self.increase_node()
                # node file handle data
                node_file__data = {
                    'name': 'file',
                    'type': 'file',
                        # 'type_code': self.node_type_code['file'],
                        'id': self.current_node_id,
                        'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                self.json_data['nodes'].append(node_file__data)
                self.nodes_existed_id[api_info] = self.current_node_id

                self.handle_to_node[graph_name+'_'+file_handle] = node_file__data
            
            connect_node = self.handle_to_node[graph_name+'_'+file_handle]
        
            # create edge from this api node to connect_node (file handle) (file node)
            if 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                self.edge(node_api__data, connect_node, {'edge_type': 'proc_file'}, graph_name)
            else:
                self.edge(connect_node, node_api__data, {'edge_type': 'proc_file'}, graph_name)

        # create edge from this api to proc node (parent_node)
        self.edge(parent_node, node_api__data, {'api_arg': api_arg, 'edge_type': 'proc_file'}, graph_name)



    def process_API_registry(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api']

        api_arg = 'NULL'
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_arg = api['flags']

        # node reg api data
        self.increase_node()
        node_api__data = {
            'name': api['api'],
            'type': 'reg_api',
                    # 'type_code': self.node_type_code['file_api'],
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

            'graph': graph_name,
            'graph_label': graph_label,
        }
        self.json_data['nodes'].append(node_api__data)
        self.nodes_existed_id[api_info] = self.current_node_id

        # if this api has key_handle, then get the key_handle, then find the node correspond with this key_handle, then connect the api_node with the key_handle node
        if 'key_handle' in api['arguments']:
            key_handle = api['arguments']['key_handle']

            # if api_name == 'NtOpenKey':
            if key_handle not in self.handle_to_node:
                # create a registry key node
                self.increase_node()
                # node key handle data
                node_reg__data = {
                    'name': 'reg',
                    'type': 'reg',
                        # 'type_code': self.node_type_code['file'],
                        'id': self.current_node_id,
                        'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                self.json_data['nodes'].append(node_reg__data)
                self.nodes_existed_id[api_info] = self.current_node_id

                self.handle_to_node[graph_name+'_'+key_handle] = node_reg__data
            
            connect_node = self.handle_to_node[graph_name+'_'+key_handle]
        
            # create edge from this api node to connect_node (file handle) (file node)
            if 'Set' in api_name or 'Write' in api_name or 'Create' in api_name:
                self.edge(node_api__data, connect_node, {'edge_type': 'proc_file'}, graph_name)
            else:
                self.edge(connect_node, node_api__data, {'edge_type': 'proc_file'}, graph_name)

        # create edge from this api to proc node (parent_node)
        self.edge(parent_node, node_api__data, {'api_arg': api_arg, 'edge_type': 'proc_file'}, graph_name)



    def edge(self, s, d, args, graph_name):
        path_data = {
            'type': args['edge_type'],
            # 'type_code': self.path_type_code[args['edge_type']],
            'args': 'NULL',
            'from': s['id'],
            'to': d['id'],
            'from_in_graph': s['id_in_graph'],
            'to_in_graph': d['id_in_graph'],

            'graph': graph_name
        }

        if args is not None and 'api_arg' in args:
            path_data['args'] = args['api_arg']
                        
        self.json_data[args['edge_type']].append(path_data)



    def encode_node(self, node):
        """
        Encode node information to node attribute
        ----------------------------
            Calculate node attributes (init features)
            All nodes must have same features space.
            Declare a space whereas:
              - 5 first elements represent node type
                  (one-hot encoding for 5 types)

              - Next 10 elements represent the node name
                  (api name / proc name) (use word embedding)

              - Next 5 element represent the path_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the regkey_written_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the dll_loaded_severity
                  for the "proc" node (one-hot encoding for 5 levels)
              - Next 5 element represent the connects_host_severity
                  for the "proc" node (one-hot encoding for 5 levels)

              - Next 5 element represent the args_severity
                  for the "api" node (one-hot encoding for 5 levels)
              - Next 5 element represent the flags_severity
                  for the "api" node (one-hot encoding for 5 levels)
        """
        # =======================================
        # Encode node name using Word Embedding
        # =======================================

        # Use name of API to represent each node
        self.nodes_labels = self.nodes_labels + [node['name']]

        ###################
        # Create graph
        ###################
        if node['graph'] not in self.graphs_dict.keys():
            self.graphs_name_to_label[node['graph']] = node['graph_label']
            self.graphs_dict[node['graph']] = DGLGraph(multigraph=True)
            self.graphs_viz[node['graph']] = Digraph(
                name=node['graph_label'], format='png')

        ###################
        # Get features
        ###################

        ndata = {}

        ''' GNN_NODE_TYPES_KEY '''
        # print('node_type_encoded', node_type_encoded)
        '''use this with node_one_hot option = True in config'''
        # nte_torch = torch.from_numpy(np.expand_dims(np.array(self.node_type_code[node['type']]), axis=0))
        '''use this with node_one_hot option = False in config'''
        node_type_encoded = indices_to_one_hot(
            self.node_type_code[node['type']], out_vec_size=len(self.node_type_code))
        nte_torch = torch.from_numpy(np.array([node_type_encoded]))
        nte_torch = torch.tensor(nte_torch).type(torch.FloatTensor)
        # print('nte_torch', nte_torch)
        ndata[GNN_NODE_TYPES_KEY] = nte_torch

        ''' GNN_NODE_LABELS_KEY '''
        cbow_node = self.cbow_encode_node_name(node['name'])
        node_embed = self.embedding(cbow_node)
        ndata[GNN_NODE_LABELS_KEY] = node_embed.view(1, -1)

        # print('ndata[GNN_NODE_LABELS_KEY]', ndata[GNN_NODE_LABELS_KEY])
        # print('ndata[GNN_NODE_TYPES_KEY]', ndata[GNN_NODE_TYPES_KEY])

        ''' GNN_NODE_ATTS_KEY '''
        # node_attr = np.concatenate([
        #     # [node['id']],
        #     [node['id_in_graph']],
        #     # node__type__encoded,
        #     [node['name']],
        #     [node['graph']],
        #     [node['graph_label']]
        # ], axis=0)
        # features = node_attr[1:-2]
        # print('features', features)
        # # features = sp.csr_matrix(features, dtype=np.float32)
        # # # features = preprocess_features(features) # sparse
        # # features = features.todense()
        # features_torch = torch.from_numpy(features)
        # features_torch = torch.tensor(
        #     features_torch).type(torch.FloatTensor)
        # # print('features_torch.shape', features_torch.shape)
        # # ndata[GNN_NODE_ATTS_KEY] = features_torch

        ''' add node with data to graph '''
        self.graphs_dict[node['graph']].add_nodes(1, data=ndata)
        self.graphs_viz[node['graph']].node(
            'n{}'.format(node['id_in_graph']), node['name'])

    def encode_edge(self, path):
        """
        Encode edge information to node attribute
        """

        if len(path) <= 0:
            del self.graphs_name_to_label[path['graph']]
            del self.graphs_dict[path['graph']]
            del self.graphs_viz[path['graph']]
            return

        self.edges_labels.append(self.path_type_code[path['type']])
        # self.edges_labels.append(path['type_code'])

        edata = {}

        ''' GNN_EDGE_TYPES_KEY '''
        '''use this with edge_one_hot = True in config'''
        # ete_torch = torch.from_numpy(np.expand_dims(np.array(self.path_type_code[path['type']]), axis=0))
        '''use this with edge_one_hot = False in config'''
        edge_type_encoded = indices_to_one_hot(
            self.path_type_code[path['type']], out_vec_size=len(self.path_type_code))
        ete_torch = torch.from_numpy(
            np.array([edge_type_encoded])).type(torch.FloatTensor)
        ete_torch = torch.tensor(ete_torch)
        # print('ete_torch', ete_torch)
        edata[GNN_EDGE_TYPES_KEY] = ete_torch

        ''' GNN_EDGE_LABELS_KEY '''
        if self.encode_edge_data is True:
            # edata[GNN_EDGE_LABELS_KEY] = path['args']
            # edata[GNN_EDGE_LABELS_KEY] = self.cbow_encode(args_to_str(path['args'])).type(torch.FloatTensor)
            cbow_edge = self.cbow_encode(args_to_str(path['args']))
            edge_embed = self.embedding(cbow_edge)
            edata[GNN_EDGE_LABELS_KEY] = edge_embed.view(1, -1)
            # print('edata[GNN_EDGE_LABELS_KEY]', edata[GNN_EDGE_LABELS_KEY])
            # print('edata[GNN_EDGE_TYPES_KEY]', edata[GNN_EDGE_TYPES_KEY])

        ''' add edge with data to graph '''
        print("path['graph']", path['graph'])
        print(self.graphs_dict[path['graph']].number_of_nodes())
        self.graphs_dict[path['graph']].add_edge(
            path['from_in_graph'], path['to_in_graph'], data=edata)
        self.graphs_viz[path['graph']].edge('n{}'.format(
            path['from_in_graph']), 'n{}'.format(path['to_in_graph']))

    def encode_data(self):
        """
        Encode nodes & edges from data.json
        """
        # first create dictionary
        is_new_dict = False
        if not os.path.exists(self.vocab_path):
            self.create_dict()
            is_new_dict = True
        # read from dict
        with open(self.vocab_path, 'r') as f:
            vocab = f.read().strip()
            self.word_dict = vocab.split(' ')
            if is_new_dict is False:
                self.append_dict()
            # vocab_size = len(vocab)
            # print('vocab size: {}'.format(vocab_size))
            self.word_to_ix = {word: i for i,
                               word in enumerate(self.word_dict)}
        # print('self.word_to_ix', self.word_to_ix)
        num_token = len(self.word_dict)
        self.embedding = nn.Embedding(num_token, 1)

        if 'nodes' in self.json_data.keys():
            n_num = 0
            n_tot = len(self.json_data['nodes'])
            # self.embed_nodes = nn.Embedding(n_tot, 1)
            print('\nencode_node')
            for node in self.json_data['nodes']:
                # print('encode_node ', node)
                self.encode_node(node)
                n_num += 1
                if n_num % 1000 == 0 or n_num == n_tot:
                    print('{}/{}'.format(n_num, n_tot))
        
        for key in self.json_data:
            if key is not 'nodes':
                p_num = 0
                p_tot = len(self.json_data[key])
                # self.embed_edges = nn.Embedding(p_tot, 1)
                print('\nencode_edge type '+key)
                for path in self.json_data[key]:
                    # print('encode_edge ', path)
                    self.encode_edge(path)
                    p_num += 1
                    if p_num % 1000 == 0 or p_num == p_tot:
                        print('{}/{}'.format(p_num, p_tot))

    def create_graphs(self):
        """
        Create graphs from encoded data to feed to network
        """
        # print(self.graphs_name_to_label)
        print('len(self.graphs_dict)', len(self.graphs_dict))
        ##############################
        # Append to graphs list
        ##############################
        gnum = 0
        for g_name in list(self.graphs_name_to_label.keys()):
            g_label = self.graphs_name_to_label[g_name]
            graph = self.graphs_dict[g_name]

            if not graph.edata:
                del self.graphs_name_to_label[g_name]
                del self.graphs_dict[g_name]

            else:
                n_nodes = graph.number_of_nodes()
                if n_nodes > self.max_n_nodes:
                    self.max_n_nodes = n_nodes

                ######################
                # Normalize edge
                ######################
                # edge_src, edge_dst = graph.edges()

                # edge_dst = list(edge_dst.data.numpy())
                # print('graph.edata ('+g_label+')', graph.edata)
                # # edge_type = list(graph.edata[GNN_EDGE_TYPES_KEY])
                # edge_lbl = list(graph.edata[GNN_EDGE_LABELS_KEY])

                # # print('edge_dst, edge_type', edge_dst, edge_type)
                # # _, inverse_index, count = np.unique((edge_dst, edge_type), axis=1, return_inverse=True, return_counts=True)
                # _, inverse_index, count = np.unique((edge_dst, edge_lbl), axis=1, return_inverse=True, return_counts=True)
                # degrees = count[inverse_index]
                # edge_norm = np.ones(
                #     len(edge_dst), dtype=np.float32) / degrees.astype(np.float32)
                # graph.edata[GNN_EDGE_NORM] = torch.FloatTensor(edge_norm)

                self.graphs.append(graph)
                self.graphs_labels.append(g_label)

                # Save this graph to png
                # if gnum < 10:
                if True:
                    # print(graph)
                    # nx.draw(graph.to_networkx(), with_labels=True)
                    # plt.savefig('data/graphs/{}.png'.format(g_name))
                    # print(self.graphs_viz[g_name].source)
                    self.graphs_viz[g_name].render(
                        filename='data/graphviz/{}'.format(g_name))
                gnum += 1

        # print(self.graphs)

        label_set = set(sorted(self.graphs_labels))  # malwar: 0, benign: 1
        num_labels = len(label_set)
        print('num_labels', num_labels)
        mapping = dict(zip(label_set, list(range(num_labels))))
        print('mapping', mapping)
        labels = [mapping[label] for label in self.graphs_labels]
        print('labels', labels)

        num_entities = len(set(self.nodes_labels))
        num_rels = len(set(self.edges_labels))

        labels_torch = torch.LongTensor(labels)

        torch.save(labels_torch, os.path.join(self.pickle_folder, LABELS))
        save_pickle(num_labels, os.path.join(self.pickle_folder, N_CLASSES))
        save_pickle(num_entities, os.path.join(self.pickle_folder, N_ENTITIES))
        save_pickle(num_rels, os.path.join(self.pickle_folder, N_RELS))
        save_pickle(self.graphs, os.path.join(self.pickle_folder, GRAPH))
        save_pickle(self.max_n_nodes, os.path.join(
            self.pickle_folder, MAX_N_NODES))
        save_pickle(self.graphs_labels, os.path.join(
            self.pickle_folder, LABELS_TXT))

        self.data_dortmund_format = {
            GRAPH: self.graphs,
            N_CLASSES: num_labels,
            N_ENTITIES: num_entities,
            N_RELS: num_rels,
            LABELS: labels_torch,
            MAX_N_NODES: self.max_n_nodes,
            LABELS_TXT: self.graphs_labels
        }

    def load_from_pickle(self):
        """
        Load data from pickle files
        """
        print(os.path.join(self.pickle_folder, GRAPH))
        self.data_dortmund_format = {
            GRAPH: load_pickle(os.path.join(self.pickle_folder, GRAPH)),
            N_CLASSES: load_pickle(os.path.join(self.pickle_folder, N_CLASSES)),
            N_ENTITIES: load_pickle(os.path.join(self.pickle_folder, N_ENTITIES)),
            N_RELS: load_pickle(os.path.join(self.pickle_folder, N_RELS)),
            LABELS: torch.load(os.path.join(self.pickle_folder, LABELS)),
            MAX_N_NODES: load_pickle(os.path.join(self.pickle_folder, MAX_N_NODES)),
            LABELS_TXT: load_pickle(os.path.join(self.pickle_folder, LABELS_TXT)),
            # GRAPH_ADJ: torch.load(os.path.join(self.pickle_folder, GRAPH_ADJ))
        }

        return self.data_dortmund_format

    def create_dict(self):
        """
        Create dictionary of name and arguments to encode
        """
        print('create_dict')
        self.word_dict.append('name NULL')
        self.append_dict()
        ''' save vocab '''
        print('save vocab')
        with open(self.vocab_path, 'w') as f:
            f.write(' '.join(self.word_dict))
    
    
    def append_dict(self):
        if 'nodes' in self.json_data.keys():
            nlen = len(self.json_data['nodes'])
            print('nlen', nlen)
            ncount = 0
            for node in self.json_data['nodes']:
                ncount += 1
                if ncount % 1000 == 0 or ncount == nlen:
                    print('update vocab at node {}/{}'.format(ncount, nlen))
                if node['name'] not in self.word_dict:
                    # print("node['name']", node['name'])
                    self.word_dict.append(node['name'])
                    
        if self.encode_edge_data is True:
            for key in self.json_data:
                plen = len(self.json_data[key])
                pcount = 0
                for path in self.json_data[key]:
                    pcount += 1
                    if pcount % 1000 == 0 or pcount == plen:
                        print('update vocab at edge type {} {}/{}'.format(key, pcount, plen))
                    txt = args_to_str(path['args'])
                    # print('txt', txt)
                    for word in txt.split(' '):
                        if word not in self.word_dict:
                            self.word_dict.append(word)
        

    def cbow_encode_node_name(self, raw_text):
        data = []
        target = raw_text
        context = ['name', target]
        # print('target', target)
        # print(self.word_to_ix[target])
        data.append((context, target))
        # print(data[0][0])
        # print(make_vector(data[0][0], self.word_to_ix))
        return make_vector(data[0][0], self.word_to_ix)

    def cbow_encode(self, raw_text):
        if len(raw_text) == 0:
            raw_text = 'NULL NULL'
        raw_text = raw_text.split(' ')
        # print('\t raw_text', raw_text)
        data = []
        i = 1
        while i < len(raw_text):
            # context = [raw_text[i - 2], raw_text[i - 1],
            #            raw_text[i + 1], raw_text[i + 2]]
            target = raw_text[i]
            context = [raw_text[i - 1], target]
            data.append((context, target))
            # print('i', i, 'context', context)
            i += 2
        # print(data)
        # print(data[0])
        # print(data[0][0])
        # print('\t data[:5]', data[:5])
        # print('make_context_vector(data[0][0], self.word_to_ix)', make_context_vector(data[0][0], self.word_to_ix))
        return make_context_vector(data[0][0], self.word_to_ix)

    # def cbow_encode(self, raw_text):
    #     if len(raw_text) == 0:
    #         raw_text = 'NULL NULL'
    #     raw_text = raw_text.split(' ')
    #     # print('\t raw_text', raw_text)
    #     data = []
    #     i = 1
    #     while i < len(raw_text):
    #         # context = [raw_text[i - 2], raw_text[i - 1],
    #         #            raw_text[i + 1], raw_text[i + 2]]
    #         context = [raw_text[i - 1]]
    #         target = raw_text[i]
    #         data.append((context, target))
    #         # print('i', i, 'context', context)
    #         i += 2
    #     print(data[0])
    #     # print('\t data[:5]', data[:5])

    #     return make_context_vector(data[0][0], self.word_to_ix)


class CBOW(nn.Module):

    def __init__(self):
        pass

    def forward(self, inputs):
        pass


def make_vector(words, word_to_ix):
    idxs = [word_to_ix[w] for w in words]
    return torch.tensor(idxs)


def make_context_vector(context, word_to_ix):
    idxs = [word_to_ix[w] for w in context]
    return torch.tensor([idxs])


def args_to_str(args_):
    str_ = str(args_)
    str_ = str_.replace('{', '').replace('}', '').replace('\'', '').replace(
        '"', '').replace(':', ' ').replace(',', ' ').replace('  ', ' ')
    return str_
